---
title: "hw5"
author: "B05611038"
date: "2018/10/18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 主題：動畫評論tf-idf與視覺化

這次是用網路爬蟲把不同話數的評論當作一個文本，嘗試以動畫「四月是你的謊言」的巴哈討論區為例，試著對各話的評論找尋一些蛛絲馬跡。

---

### 網路爬蟲

```
library('xml2')
library('rvest')
#library of web crawling
library('stringr')
#library of string process

numextract <- function(string){
  str_extract(string, "\\-*\\d+\\.*\\d*")
}
#the function used in greb number from string

URL.origin <- "https://forum.gamer.com.tw/"
URL.part1 <- "https://forum.gamer.com.tw/B.php?page="
URL.part2 <- "&bsn=44095"
#the target forum

title <- c()
link <- c()
page <- 1
check <- character(0)
while(TRUE) {
  URL <- paste(URL.part1, toString(page), URL.part2, sep = "")
  #we can get a iterable webpage URL

  title.temp <- read_html(URL) %>% html_nodes(".b-list__main__title") %>% html_text()
  link.temp <- read_html(URL) %>% html_nodes(".b-list__main__title") %>% xml_attr("href")
  #get the title of the each small part for our next step

  if (identical(title.temp, check) || identical(title.temp, check)) {
    break
  }

  title <- c(title, title.temp)
  link <- c(link, link.temp)

  page <- page + 1
}

check <- integer(0)
index <- c()
for (i in c(1: length(title))) {
  index.temp <- grep("集中討論串", title[i])
  #to check if the link we wants to check
  if (identical(index.temp, check)) {
    #don't do anything
  } else {
    index <- c(index, i)
    #to save the link we wants to go in
  }
}

lost.part <- "?page="
check <- character(0)
#data <- list()
data <- c()
for (i in c(1: length(index))) {
  cat("Start process page number ", i, "\n")
  content <- c()
  page <- 1
  link.temp <- link[index[i]]
  filename <- numextract(title[index[i]])
  filename <- paste("episode", filename, ".txt", sep = "")
  #to extract the sublink we need to use
  link.temp <- unlist(strsplit(link.temp, split = "?", fixed = TRUE))
  while(TRUE) {
    URL <- paste(URL.origin, link.temp[1], lost.part, toString(page), toString("&"), link.temp[2], sep = "")
    content.temp <- read_html(URL) %>% html_nodes(".c-article__content") %>% html_text()
    if (identical(content.temp, check)) {
      break
    }

    content <- c(content, content.temp)
    #to concatenate the content in all the page

    check <- content.temp
    #to let the check become the same as last page
    content <- str_replace_all(content, " ", "")
    content <- str_replace_all(content, "  ", "")
    content <- str_replace_all(content, "\n", "")
    content <- str_replace_all(content, "　", "")

    page <- page + 1
  }
  #the loop will end in the grab all page
  #write.table(content, filename, sep = "")
  #cat("Output file: ", filename, " success\n")
  content.temp <- content[1]
  for (x in c(2: length(content))) {
    content.temp <- paste(content.temp, content[x], sep = "")
  }
  data <- c(data, content.temp)
}
```

```
    這段程式其實是抓取討論區中各個不同的話的所有評論當作一個文本，並且存成一個.txt檔，因為是從最大的文章列表進行抓取，針對「集中討論區」也就是動畫各話的評論進行抓取，為了之後處理tf-idf的方便性，我們預先將所有不同的評論存成一個string，因為四月是你的謊言總共有22話 + 1話的OAD，所以至少會因為「集中討論區」的標題抓取抓到23個文本
```

---

### Tf-idf

因為資料量也不小，所以就不在Rmd中顯示了，若要看結果的話，請去下載show.csv檔

```
library('NLP')
library('tm')
library('SnowballC')
library('RColorBrewer')
library('jiebaRD')
library('jiebaR')
library('slam')
library('Matrix')
library('tidytext')
#library of text mining

cc <- worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]], cc))
}
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))
})
#the delimeter of chinese
docs <- Corpus(VectorSource(data))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)

seq = c()
for (i in c(1: length(index))) {
  seq = c(seq, jieba_tokenizer(data[i]))
}
freqFrame = as.data.frame(table(unlist(seq)))
d.corpus <- Corpus(VectorSource(seq))

dtm <- TermDocumentMatrix(d.corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing = TRUE)
d <- data.frame(word = names(v),freq = v)
DF <- tidy(m)

N = dtm$ncol
tf <- apply(dtm, 2, sum)
idfCal <- function(word_doc)
{
  log2( N / nnzero(word_doc) )
}
idf <- apply(dtm, 1, idfCal)

doc.tfidf <- as.matrix(dtm)
for(x in 1:nrow(dtm)) {
  for(y in 1:ncol(dtm)) {
    doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
  }
}

findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
tfidfnn = doc.tfidf[-which(findZeroId == 0),]

tfidfnn

write.csv(tfidfnn, "show.csv")
```

```
    這段就是根據不同文本，先將所有文本合成一個巨大的文本進行斷詞，可以得出約30000個不同的詞，也就是我們平常在說的term，並且根據tf-idf，也就是高頻率的詞在文本中可以認為是重要的，但是像是「的」、「都」這種高頻率出現在中文的字就會被過度放大，這就是tf的部分，而所謂idf就是指這些常常使用的字在各個文本出現的頻率，將這兩個東西相乘之後，就可以得到一個兩個不同概念所成的拮抗的數值，而tf-idf值越高代表在這個文本中這個字越重要，雖然這是一個根據簡單機率算出來的一個很模糊的值，但這的確在資料量夠大的時候，可以有更高的機率看出一個詞的重要性。
```

---

### 結語

其實這次感覺還比較像爬蟲練習XD，我個人認為tf-idf的代表意義實在太粗糙了，但是這次網路爬蟲真的練習到了不少，怎麼從一堆link中探詢子分頁的關係，之後就成功抓取自己要的資料了，可是其實觀察這個很大的csv還是很難得出結果，只能認為評論的詞都太分散了。
